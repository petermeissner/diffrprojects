% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/text_tools.R
\name{text_tokenize}
\alias{text_tokenize}
\title{function to tokenize text}
\usage{
text_tokenize(x, regex = NULL, ignore.case = FALSE, fixed = FALSE,
  useBytes = FALSE, non_token = FALSE)
}
\arguments{
\item{x}{character vector to be tokenized}

\item{regex}{regex to use for tokenization}

\item{ignore.case}{see \link{grep}, interanlly passed through to gregexpr()}

\item{fixed}{see \link{grep}, interanlly passed through to gregexpr()}

\item{useBytes}{see \link{grep}, interanlly passed through to gregexpr()}

\item{group}{predefined regular expressions}
}
\value{
data.frame,
   token: string of the token;
   from: position in text at which token starts;
   to: position in text at which the token ends
   length: length of the token;
   type: type of the token, either its matched by regular expression used for tokenization or not matched
}
\description{
function to tokenize text
}

